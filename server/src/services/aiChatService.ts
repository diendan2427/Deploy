import axios from 'axios';
import ChatHistory from '../models/chatHistory.model';
import TrainingData from '../models/trainingData.model';
import Challenge from '../models/challenge.model';
import Submission from '../models/submission.model';
import User from '../models/user.model';
import mongoose from 'mongoose';

// ============================================
// CONFIG
// ============================================
const GEMINI_API_URL = 'https://generativelanguage.googleapis.com/v1/models';
const GEMINI_API_KEY = process.env.GEMINI_API_KEY || process.env.GOOGLE_GEMINI_API_KEY || '';
const GEMINI_MODEL = process.env.GEMINI_MODEL || 'gemini-1.5-flash';

// Cache ƒë·ªÉ tr√°nh fetch models nhi·ªÅu l·∫ßn
let cachedModel: string | null = null;

// ============================================
// SYSTEM PROMPT (T√πy ch·ªânh theo d·ª± √°n)
// ============================================
const SYSTEM_PROMPT = `B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán v√† chuy√™n nghi·ªáp c·ªßa BugHunter - m·ªôt n·ªÅn t·∫£ng h·ªçc l·∫≠p tr√¨nh th√¥ng qua vi·ªác s·ª≠a l·ªói code.

NHI·ªÜM V·ª§:
- Ph√¢n t√≠ch d·ªØ li·ªáu v√† ƒë∆∞a ra l·ªùi khuy√™n c·ª• th·ªÉ v·ªÅ l·∫≠p tr√¨nh
- Gi√∫p ng∆∞·ªùi d√πng hi·ªÉu r√µ th√¥ng tin c·ªßa h·ªç (b√†i t·∫≠p, th·ª≠ th√°ch, ti·∫øn ƒë·ªô h·ªçc t·∫≠p)
- ƒê·ªÅ xu·∫•t gi·∫£i ph√°p ph√π h·ª£p cho c√°c v·∫•n ƒë·ªÅ l·∫≠p tr√¨nh
- H·ªó tr·ª£ debug code v√† gi·∫£i th√≠ch c√°c kh√°i ni·ªám l·∫≠p tr√¨nh

PHONG C√ÅCH:
- N√≥i ti·∫øng Vi·ªát t·ª± nhi√™n, g·∫ßn g≈©i
- D√πng emoji ph√π h·ª£p (üí°, üìä, üéØ, ‚ú®, üêõ, üíª)
- Lu√¥n l·∫°c quan v√† ƒë·ªông vi√™n
- ƒê∆∞a ra l·ªùi khuy√™n c·ª• th·ªÉ v·ªõi s·ªë li·ªáu r√µ r√†ng

NGUY√äN T·∫ÆC:
- Kh√¥ng ƒë·ªÅ c·∫≠p th√¥ng tin nh·∫°y c·∫£m
- Kh√¥ng khuy√™n ng∆∞·ªùi d√πng l√†m ƒëi·ªÅu r·ªßi ro
- Lu√¥n khuy·∫øn kh√≠ch h√†nh vi t√≠ch c·ª±c
- T·∫≠p trung v√†o vi·ªác h·ªçc l·∫≠p tr√¨nh v√† c·∫£i thi·ªán k·ªπ nƒÉng`;

// ============================================
// HELPER: T·ª± ƒë·ªông ch·ªçn model ·ªïn ƒë·ªãnh
// ============================================
async function getAvailableModel(): Promise<string> {
  if (cachedModel) return cachedModel;
  
  try {
    console.log('[AIChatService] Fetching available Gemini models...');
    const response = await axios.get(
      `${GEMINI_API_URL}?key=${GEMINI_API_KEY}`,
      { timeout: 10000 }
    );
    
    if (response.data?.models) {
      const models = response.data.models;
      
      // ∆Øu ti√™n model ·ªïn ƒë·ªãnh (tr√°nh beta/experimental)
      const stable = models.find((m: any) => 
        m.name.includes('gemini-1.5-flash') && 
        !m.name.includes('2.5') &&
        m.supportedGenerationMethods?.includes('generateContent')
      );
      
      cachedModel = stable?.name || models.find((m: any) => 
        m.supportedGenerationMethods?.includes('generateContent')
      )?.name;
      
      if (cachedModel) {
        console.log(`[AIChatService] Using model: ${cachedModel}`);
        return cachedModel;
      }
    }
    
    throw new Error('No available models found');
  } catch (error: any) {
    console.error('[AIChatService] Error fetching models:', error.message);
    cachedModel = `models/${GEMINI_MODEL}`;
    return cachedModel;
  }
}

// ============================================
// HELPER: G·ªçi Gemini API
// ============================================
async function callGeminiAPI(prompt: string): Promise<string> {
  try {
    // Check if API key is configured
    if (!GEMINI_API_KEY || GEMINI_API_KEY.trim() === '') {
      console.warn('[AIChatService] GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh, s·ª≠ d·ª•ng fallback response');
      return 'Xin l·ªói, h·ªá th·ªëng AI ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n. üôè';
    }

    const modelName = await getAvailableModel();
    const modelPath = typeof modelName === 'string' ? modelName : modelName;
    const cleanModelName = modelPath.replace('models/', '');
    
    const url = `${GEMINI_API_URL}/${cleanModelName}:generateContent?key=${GEMINI_API_KEY}`;
    
    const response = await axios.post(
      url,
      {
        contents: [{
          parts: [{ text: prompt }]
        }],
        generationConfig: {
          temperature: 0.7,      // Creativity (0-1)
          maxOutputTokens: 4096, // Max response length
        }
      },
      {
        headers: { 'Content-Type': 'application/json' },
        timeout: 30000
      }
    );

    // Parse response
    if (response.data?.candidates?.[0]) {
      const candidate = response.data.candidates[0];
      
      // Handle safety blocks
      if (candidate.finishReason === 'SAFETY' || candidate.finishReason === 'RECITATION') {
        return 'Xin l·ªói, c√¢u h·ªèi kh√¥ng ph√π h·ª£p v·ªõi ch√≠nh s√°ch an to√†n. üôè';
      }
      
      // Get text response
      if (candidate.content?.parts?.[0]?.text) {
        return candidate.content.parts[0].text;
      }
    }

    console.error('[AIChatService] Invalid Gemini response:', response.data);
    return 'Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi n√†y. H√£y th·ª≠ l·∫°i! üòä';
    
  } catch (error: any) {
    console.error('[AIChatService] Gemini API call failed:', error.message);
    
    // Provide user-friendly error messages
    if (error.response?.status === 401 || error.response?.status === 403) {
      return 'Xin l·ªói, API key kh√¥ng h·ª£p l·ªá. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n. üîë';
    } else if (error.response?.status === 429) {
      return 'Xin l·ªói, h·ªá th·ªëng ƒëang qu√° t·∫£i. Vui l√≤ng th·ª≠ l·∫°i sau v√†i gi√¢y. ‚è≥';
    } else if (error.code === 'ECONNREFUSED' || error.code === 'ETIMEDOUT') {
      return 'Xin l·ªói, kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn d·ªãch v·ª• AI. Vui l√≤ng th·ª≠ l·∫°i sau. üåê';
    }
    
    throw error;
  }
}

// ============================================
// MAIN SERVICE CLASS
// ============================================
class AIChatService {
  
  // L·∫•y context t·ª´ database (T√πy ch·ªânh theo d·ª± √°n)
  async getUserContext(userId: string) {
    try {
      const user = await User.findById(userId).lean();
      if (!user) {
        return {
          hasData: false,
          message: 'Ch∆∞a c√≥ d·ªØ li·ªáu ng∆∞·ªùi d√πng'
        };
      }

      // L·∫•y th·ªëng k√™ c·ªßa user
      const completedChallenges = await Challenge.countDocuments({
        _id: { $in: user.completedChallenges || [] }
      });

      const totalSubmissions = await Submission.countDocuments({ userId: new mongoose.Types.ObjectId(userId) });

      return {
        hasData: true,
        data: {
          experience: user.experience || 0,
          rank: user.rank || 'Newbie',
          completedChallenges,
          totalSubmissions,
          badges: user.badges?.length || 0
        }
      };
    } catch (error) {
      console.error('[AIChatService] Error getting user context:', error);
      return { hasData: false, message: 'Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu' };
    }
  }

  // Format context th√†nh text cho AI
  formatContext(context: any): string {
    if (!context.hasData) return context.message;
    
    const { data } = context;
    return `Th√¥ng tin ng∆∞·ªùi d√πng:
- ƒêi·ªÉm kinh nghi·ªám: ${data.experience}
- H·∫°ng: ${data.rank}
- S·ªë b√†i t·∫≠p ƒë√£ ho√†n th√†nh: ${data.completedChallenges}
- T·ªïng s·ªë l·∫ßn submit: ${data.totalSubmissions}
- S·ªë huy hi·ªáu: ${data.badges}`;
  }

  // L·∫•y l·ªãch s·ª≠ chat (gi·ªõi h·∫°n 3 tin nh·∫Øn g·∫ßn nh·∫•t)
  async getChatHistory(userId: string, limit: number = 3) {
    try {
      const chatHistory = await ChatHistory.findOne({ userId: new mongoose.Types.ObjectId(userId) })
        .sort({ updatedAt: -1 })
        .lean();

      if (!chatHistory || !chatHistory.messages) {
        return [];
      }

      // L·∫•y tin nh·∫Øn g·∫ßn nh·∫•t (lo·∫°i b·ªè system messages)
      const recentMessages = chatHistory.messages
        .filter(msg => msg.role !== 'system')
        .slice(-limit)
        .map(msg => ({
          role: msg.role,
          content: msg.content
        }));

      return recentMessages;
    } catch (error) {
      console.error('[AIChatService] Error getting chat history:', error);
      return [];
    }
  }

  // G·ª≠i tin nh·∫Øn v√† nh·∫≠n response
  async sendMessage(userId: string, userMessage: string) {
    try {
      // 1. L·∫•y context t·ª´ DB
      const context = await this.getUserContext(userId);
      const contextText = this.formatContext(context);
      
      // 2. L·∫•y l·ªãch s·ª≠ chat
      const history = await this.getChatHistory(userId, 3);
      
      // 3. Build prompt
      let fullPrompt = SYSTEM_PROMPT + '\n\n';
      
      if (context.hasData) {
        fullPrompt += `Context:\n${contextText}\n\n`;
      }
      
      if (history.length > 0) {
        fullPrompt += 'Recent chat:\n';
        history.forEach(msg => {
          fullPrompt += `${msg.role}: ${msg.content}\n`;
        });
        fullPrompt += '\n';
      }
      
      fullPrompt += `User: ${userMessage}\n\nAssistant:`;
      
      // 4. Save user message to chat history (using ChatHistory model)
      const userIdObjectId = new mongoose.Types.ObjectId(userId);
      let chatHistory = await ChatHistory.findOne({ userId: userIdObjectId })
        .sort({ updatedAt: -1 });
      
      if (!chatHistory) {
        chatHistory = new ChatHistory({
          userId: userIdObjectId,
          messages: [],
          title: userMessage.substring(0, 50)
        });
      }
      
      chatHistory.messages.push({
        role: 'user',
        content: userMessage,
        timestamp: new Date()
      });
      
      // 5. Call AI
      const aiResponse = await callGeminiAPI(fullPrompt);
      
      // 6. Save AI response to chat history
      chatHistory.messages.push({
        role: 'assistant',
        content: aiResponse,
        timestamp: new Date()
      });
      
      await chatHistory.save();
      
      // Also save to ChatMessage model for individual message tracking
      try {
        const ChatMessageModule = await import('../models/chatMessage.model');
        const ChatMessage = ChatMessageModule.default;
        await ChatMessage.create({
          userId: userIdObjectId,
          role: 'user',
          content: userMessage,
          metadata: {
            contextUsed: context.hasData,
            timestamp: new Date()
          }
        });
        await ChatMessage.create({
          userId: userIdObjectId,
          role: 'assistant',
          content: aiResponse,
          metadata: {
            contextUsed: context.hasData,
            timestamp: new Date(),
            model: GEMINI_MODEL
          }
        });
      } catch (msgError: any) {
        console.error('[AIChatService] Error saving to ChatMessage:', msgError?.message || msgError);
        // Continue even if ChatMessage save fails
      }
      
      return {
        success: true,
        message: aiResponse,
        hasContext: context.hasData
      };
      
    } catch (error: any) {
      console.error('[AIChatService] Error in sendMessage:', error);
      
      return {
        success: false,
        message: 'Xin l·ªói, ƒë√£ x·∫£y ra l·ªói. Vui l√≤ng th·ª≠ l·∫°i! üòî'
      };
    }
  }

  // X√≥a l·ªãch s·ª≠ chat
  async clearHistory(userId: string) {
    try {
      const userIdObjectId = new mongoose.Types.ObjectId(userId);
      await ChatHistory.deleteMany({ userId: userIdObjectId });
      
      // Also clear ChatMessage records
      try {
        const ChatMessageModule = await import('../models/chatMessage.model');
        const ChatMessage = ChatMessageModule.default;
        await ChatMessage.deleteMany({ userId: userIdObjectId });
      } catch (msgError: any) {
        console.error('[AIChatService] Error clearing ChatMessage:', msgError?.message || msgError);
      }
      
      return { success: true, message: 'ƒê√£ x√≥a l·ªãch s·ª≠' };
    } catch (error) {
      console.error('[AIChatService] Error clearing history:', error);
      return { success: false, message: 'Kh√¥ng th·ªÉ x√≥a l·ªãch s·ª≠' };
    }
  }
}

export default new AIChatService();

